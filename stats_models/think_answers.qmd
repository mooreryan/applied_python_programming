---
author: "Ryan M. Moore, PhD"
date-modified: last-modified
date: "2025-04-24"
jupyter: python3
---

# Stop & Think Answers — @sec-stats-models

@tip-08-pvalue-misconceptions: Relying solely on p-values can lead to overinterpreting small, biologically meaningless differences as important simply because they're statistically significant, especially with large sample sizes.

@tip-08-parametric-vs-nonparametric: You might choose a nonparametric test like Mann-Whitney when your data doesn't follow a normal distribution or when you have small sample sizes and can't verify distributional assumptions.

@tip-08-which-anova: This is definitely a personal preference type of questions! But I kind of like the 2nd option. It's a bit "fancy" but it's nice because it only uses data contained in the data frame itself and doesn't require going through the keys in the `REGIONS` map.

@tip-08-statistical-vs-biological: A result can be statistically significant but have such a small effect size that it's biologically meaningless, or a result can have a large biological effect but fail to reach statistical significance due to small sample size.

@tip-08-effect-size-examples: In drug studies, a medication might show a statistically significant reduction in some biomarker (p < 0.05), but the actual change might be so small (tiny effect size) that it doesn't translate to any meaningful clinical improvement for patients.

@tip-08-post-hoc-interpretation: Regions whose confidence intervals don't overlap have significantly different cancer death rates. From the plot, it appears the West has significantly lower death rates than several other regions, particularly the Southeast.

@tip-08-regression-interpretation: An R-squared value close to zero indicates that the linear model explains almost none of the variation in the dependent variable, suggesting there's no linear relationship between the predictor and response variables.

@tip-08-feature-selection: You can determine important predictors by examining their coefficients and p-values in the summary output. Predictors with larger absolute coefficient values and p-values < 0.05 (like X1 and X2 in our example) are more important to the model.

@tip-08-pca-interpretation: Although we didn’t go over the biplot, it can be a useful tool for interpreting the principal components. That said, you can still examine the loadings returned by the PCA model to understand how each feature relates to PC 1 (the x-axis). Specifically, you can compute the angle each loading vector makes with the x-axis. Features with small angles (or angles close to 180°) align closely with PC 1. For example, you could use something like `result.loadings.apply(lambda row: np.arctan2(row[1], row[0]) * 180 / np.pi, axis=1)` to calculate these angles. In this dataset, petal length and petal width are most strongly associated with PC 1.

@tip-08-clustering-challenges: One potential explanation that k-means performed less well for _virginica_ is because it overlaps more with _versicolor_ in the feature space, while _setosa_ is more distinct. This suggests _virginica_ and _versicolor_ share more similar morphological characteristics.
