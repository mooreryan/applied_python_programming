---
author: "Ryan M. Moore, PhD"
date-modified: last-modified
date: "2025-03-24"
jupyter: python3
---

# Solutions

## Chapter 6: Errors & Exceptions

### Stop & Think

@tip-06-name-error: When Python tries to evaluate the name `gene` and doesn't find it in any scope, it raises a `NameError`, which matches the exception type specified in the except clause. This causes a message with the error details to be printed to the console.

@tip-06-file-not-exist: `FileNotFoundError`

@tip-06-handle-na: We could check if the expression value is "na" before trying to convert it, or use a try/except block to catch the ValueError and set a default value (like `None`, `1`, `0`, or `NaN`).

@tip-06-exception-chaining: When analyzing sequencing datasets, one error might trigger others in a cascade. For example, a file reading error might lead to missing data, which then causes calculation errors. This chain makes it harder to find the root cause of the error.

@tip-06-file-read-errors: Specific exceptions might include: `FileNotFoundError`, `PermissionError`, `IsADirectoryError`. All of these could be caught by `OSError`, which is the parent class for file-related errors.

@tip-06-finally-clause: The `finally` clause is useful when working with resources that need to be released regardless of success or failure, such as closing file handles or database connections.

@tip-06-custom-exceptions: Possible custom exceptions: `InvalidSequenceError`, `AlignmentFailedError`, `LowCoverageError`, `DifferentialExpressionError`, etc.

### Practice Problems

#### Solution @sec-problem-6.1

```python
try:
    value = float("abc")
except ValueError:
    print("Not a valid number")
```

#### Solution @sec-problem-6.2

```python
counts = {"A": 1, "C": 2, "G": 0, "T": 4}
total = sum(counts.values())

try:
    n_ratio = counts["N"] / total
except KeyError:
    print("N is not present in the counts dictionary")
    n_ratio = None
```

#### Solution @sec-problem-6.3

```python
try:
    silly_divide(5, 0)
except ZeroDivisionError:
    print("you can't divide by zero!")
except Exception as error:
    print(f"a mysterious error occurred: {error=}")
```

#### Solution @sec-problem-6.4

```python
def fold_change(expression_1, expression_2)
    try:
        return expression_1 / expression_2
    except ZeroDivisionError:
        print("expression_2 was zero!")
        return None
```

#### Solution @sec-problem-6.5

```python
class SequenceLengthError(Exception):
    pass

MIN_LENGTH = 50
MAX_LENGTH = 150

def validate_sequence_length(sequence):
    sequence_length = len(sequence)

    if sequence_length < MIN_LENGTH:
        raise SequenceLengthError(f"sequence length {sequence_length} was too short!")

    if sequence_length > MAX_LENGTH:
        raise SequenceLengthError(f"sequence length {sequence_length} was too long!")

    return None
```

#### Solution @sec-problem-6.6

```python
def run_simulation(max_turns):
    if max_turns < 1:
        raise ValueError(f"Expected at least 1 iteration, but got {max_turns=}")

    if max_turns > 1000:
        raise ValueError(f"Expected at most 1000 iterations, but got {max_turns=}")

    # Simulation code would follow
    pass
```

## Chapter 7: Intro to Exploratory Data Analysis with Python

### Stop & Think

@tip-07-why-summarize-data response:  Looking at a summary of your data has a ton of benefits.  Summarizing your data can help you:

- Identify data import issues, like importing the wrong file
- Identify data quality issues, like missing values or outliers
- Become familiar with the data, including data types and ranges

The more familiar you are with your data, the better!

@tip-07-why-nice-plots response: Clean visualizations aren’t just important for presenting data to others, they are also important for presenting data to yourself!  A messy graph may hide trends, overload the viewer, or be otherwise difficult to interpret, potentially leading to misinterpretations or missed patterns.  Professional-looking visualizations can also be shared with colleagues through presentations and reports with minimal modifications, freeing up time later.  Lastly, it’s never bad to get in the habit of producing clear and effective visualizations of your data!

@tip-07-good-color-palette response: Choosing a good color palette is critical for communicating information about your data.  Improper color choice can easily lead to misinterpretations.  In the heatmap example, we use a diverging color scheme to show the difference between positive and negative correlations, but the balance of those colors must be correct:

- If the center of the palette was on 0.2 rather than zero, it would visually suggest that values around 0.2 are "neutral" or "average,” rather than “positive”
- With asymmetric color intensity (brightest blue at -0.2, brightest orange at 1.0), it would create visual bias, making positive correlations appear weaker than negative ones of the same magnitude

In both cases, viewers would easily be thrown off by the color scheme, even with a legend available.

@tip-07-why-filter-columns response: Filtering columns by category can help manage cognitive load and distraction when working with large datasets, allowing you to focus on particular variables. It can make it easier to identify patterns, facilitate more targeted analysis, and help build intuition about how different aspects of the data relate to each other.

@tip-07-what-correlation-tells-us response: Correlation values can show potential relationships between variables and suggest further avenues for investigation. However, they have limitations: they only measure linear relationships, can be heavily influenced by outliers, and don't indicate causation. They should be considered as starting points for deeper analysis rather than conclusive findings.

@tip-07-log-scale-purpose response: While we are most used to linear scales, logarithmic scales are useful in many situations:

⁃ When data spans several orders of magnitude, log scales lett you visualize both small and large values effectively on the same plot
⁃ When data follows exponential growth patterns, like in the early stages of an [epidemic](https://data.europa.eu/apps/data-visualisation-guide/logarithmic-y-axes)
⁃ When looking for proportional or percentage changes rather than count changes, like in the case of this [library example](https://www.lrs.org/2020/06/17/visualizing-data-the-logarithmic-scale/)

In fact, visualizing data on a logarithmic scale can sometimes reveal trends that are not apparent from count data, as in [this example using cancer data](https://www.pa.gov/content/dam/copapwp-pagov/en/health/documents/topics/healthstatistics/statistical-resources/understandinghealthstats/documents/Arithmetic_vs_Logarithmic_Line_Charts.pdf).

Logarithmic transformations also have some nifty statistical applications, like [reducing skew in data](https://pmc.ncbi.nlm.nih.gov/articles/PMC9036143/).

@tip-07-why-correlation-heatmap response: Heatmaps add an extra visual layer that is not present in tables (color), making strong correlations visually obvious.  Clustering reveals groups of variables that behave similarly, and the dendrograms show how those variables are related to each other.  Together, these components make patterns easier to spot than a table alone and add hierarchical structure that can’t be easily represented in a data table.

@tip-07-merge-considerations response: When merging datasets, it’s important to consider:

1. Whether incomplete data is acceptable or problematic.  If problematic, choose an inner join.
2. If either dataset is primary to your analysis.  If no, choose an outer join.  If yes, choose a left or right join according to the dataset.

Along the way, consider how representative the resulting dataset will be and whether you will be able to use it effectively for any planned analyses or visualizations.

### Practice Problems

#### Solution @sec-problem-7.1

```python
df = pd.DataFrame(state_cancer_data)
```

#### Solution @sec-problem-7.2

```python
df["Cancer Deaths Per 100k"] = df["Cancer Deaths"] / df["Population"] * 100_000
```

#### Solution @sec-problem-7.3

```python
df.query("`Cancer Deaths Per 100k` >= 180 and `Median Household Income` < 68_500")
```

#### Solution @sec-problem-7.4

```python
df.plot(kind="scatter", x="Median Household Income", y="Cancer Deaths Per 100k")
```

#### Solution @sec-problem-7.5

```python
df.plot(kind="scatter", x="Percent Aged 65+", y="Cancer Deaths Per 100k")
```

## Chapter 8: Intro to Statistics & Modeling with Python

### Stop and Think

@tip-08-pvalue-misconceptions: Relying solely on p-values can lead to overinterpreting small, biologically meaningless differences as important simply because they're statistically significant, especially with large sample sizes.

@tip-08-parametric-vs-nonparametric: You might choose a nonparametric test like Mann-Whitney when your data doesn't follow a normal distribution or when you have small sample sizes and can't verify distributional assumptions.

@tip-08-which-anova: This is definitely a personal preference type of questions! But I kind of like the 2nd option. It's a bit "fancy" but it's nice because it only uses data contained in the data frame itself and doesn't require going through the keys in the `REGIONS` map.

@tip-08-statistical-vs-biological: A result can be statistically significant but have such a small effect size that it's biologically meaningless, or a result can have a large biological effect but fail to reach statistical significance due to small sample size.

@tip-08-effect-size-examples: In drug studies, a medication might show a statistically significant reduction in some biomarker (p < 0.05), but the actual change might be so small (tiny effect size) that it doesn't translate to any meaningful clinical improvement for patients.

@tip-08-post-hoc-interpretation: Regions whose confidence intervals don't overlap have significantly different cancer death rates. From the plot, it appears the West has significantly lower death rates than several other regions, particularly the Southeast.

@tip-08-regression-interpretation: An R-squared value close to zero indicates that the linear model explains almost none of the variation in the dependent variable, suggesting there's no linear relationship between the predictor and response variables.

@tip-08-feature-selection: You can determine important predictors by examining their coefficients and p-values in the summary output. Predictors with larger absolute coefficient values and p-values < 0.05 (like X1 and X2 in our example) are more important to the model.

@tip-08-pca-interpretation: Although we didn’t go over the biplot, it can be a useful tool for interpreting the principal components. That said, you can still examine the loadings returned by the PCA model to understand how each feature relates to PC 1 (the x-axis). Specifically, you can compute the angle each loading vector makes with the x-axis. Features with small angles (or angles close to 180°) align closely with PC 1. For example, you could use something like `result.loadings.apply(lambda row: np.arctan2(row[1], row[0]) * 180 / np.pi, axis=1)` to calculate these angles. In this dataset, petal length and petal width are most strongly associated with PC 1.

@tip-08-clustering-challenges: One potential explanation that k-means performed less well for _virginica_ is because it overlaps more with _versicolor_ in the feature space, while _setosa_ is more distinct. This suggests _virginica_ and _versicolor_ share more similar morphological characteristics.

### Practice Problems

The solutions use the following imports:

```{python}
import numpy as np
import pandas as pd
import scipy.cluster as cluster
import scipy.stats as stats
import seaborn as sns
import statsmodels.formula.api
```

#### Solution @sec-problem-8.1

```{python}
np.random.seed(2503478)
group_A = stats.norm(loc=10, scale=2).rvs(30)
group_B = stats.norm(loc=15, scale=2).rvs(30)
result = stats.ttest_ind(group_A, group_B)
print(result)
```

#### Solution @sec-problem-8.2

```{python}
np.random.seed(493567)
group_A = stats.norm(loc=10, scale=2).rvs(30)
group_B = stats.norm(loc=15, scale=2).rvs(30)
group_C = stats.norm(loc=11, scale=2).rvs(30)
result = stats.f_oneway(group_A, group_B, group_C)
print(result)
```

#### Solution @sec-problem-8.3

```{python}
result = stats.tukey_hsd(group_A, group_B, group_C)
print(result)
```

#### Solution @sec-problem-8.4

```{python}
np.random.seed(932847)
x1 = np.random.uniform(-10, 10, 50)
x2 = np.random.uniform(-10, 10, 50)
y = 3 * x1 + np.random.normal(0, 2, 50)
df = pd.DataFrame({"X1": x1, "X2": x2, "Y": y})
model = statsmodels.formula.api.ols("Y ~ X1 + X2", data=df).fit()
print(model.summary().tables[1])
```

#### Solution @sec-problem-8.5

Two clusters mainly separate _setosa_ from _versicolor_ and _virginica_, which makes biological sense given that _setosa_ is the most distinct species.

```{python}
iris = pd.read_csv("../_data/iris.csv")
sns.pairplot(iris, hue="Species")
_centroids, labels = cluster.vq.kmeans2(iris.drop(columns="Species"), k=2)
sns.pairplot(iris.assign(Cluster=labels), hue="Cluster")
```

## Chapter 9

### Stop & Think

@tip-09-missing-data: This file no longer included `Hello, this is my first file!` because the write mode (`"w"`) will overwrite the contents of existing files by default.

@tip-09-improving-error-messages: You could include the name of the file that caused the error.

@tip-09-parse-fasta-fastq: It won't work correctly! At the time of writing this chapter, biopython gives a warning about the file format.

@tip-09-csv-no-header: If the `fieldnames` argument is omitted, the values in the first row of the file will be used as the field names and will be omitted from the results.
