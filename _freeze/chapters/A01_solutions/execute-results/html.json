{
  "hash": "3cbfa1a2faa0d990764cb68fc87562d8",
  "result": {
    "engine": "jupyter",
    "markdown": "---\nauthor: \"Ryan M. Moore, PhD\"\ndate-modified: last-modified\ndate: \"2025-03-24\"\njupyter: python3\n---\n\n\n\n\n# Appendix 1: Solutions {.unnumbered}\n\n## Chapter 6: Errors & Exceptions\n\n### Stop & Think\n\n@tip-06-name-error: When Python tries to evaluate the name `gene` and doesn't find it in any scope, it raises a `NameError`, which matches the exception type specified in the except clause. This causes a message with the error details to be printed to the console.\n\n@tip-06-file-not-exist: `FileNotFoundError`\n\n@tip-06-handle-na: We could check if the expression value is \"na\" before trying to convert it, or use a try/except block to catch the ValueError and set a default value (like `None`, `1`, `0`, or `NaN`).\n\n@tip-06-exception-chaining: When analyzing sequencing datasets, one error might trigger others in a cascade. For example, a file reading error might lead to missing data, which then causes calculation errors. This chain makes it harder to find the root cause of the error.\n\n@tip-06-file-read-errors: Specific exceptions might include: `FileNotFoundError`, `PermissionError`, `IsADirectoryError`. All of these could be caught by `OSError`, which is the parent class for file-related errors.\n\n@tip-06-finally-clause: The `finally` clause is useful when working with resources that need to be released regardless of success or failure, such as closing file handles or database connections.\n\n@tip-06-custom-exceptions: Possible custom exceptions: `InvalidSequenceError`, `AlignmentFailedError`, `LowCoverageError`, `DifferentialExpressionError`, etc.\n\n### Practice Problems\n\n#### Solution @sec-problem-6.1\n\n```python\ntry:\n    value = float(\"abc\")\nexcept ValueError:\n    print(\"Not a valid number\")\n```\n\n#### Solution @sec-problem-6.2\n\n```python\ncounts = {\"A\": 1, \"C\": 2, \"G\": 0, \"T\": 4}\ntotal = sum(counts.values())\n\ntry:\n    n_ratio = counts[\"N\"] / total\nexcept KeyError:\n    print(\"N is not present in the counts dictionary\")\n    n_ratio = None\n```\n\n#### Solution @sec-problem-6.3\n\n```python\ntry:\n    silly_divide(5, 0)\nexcept ZeroDivisionError:\n    print(\"you can't divide by zero!\")\nexcept Exception as error:\n    print(f\"a mysterious error occurred: {error=}\")\n```\n\n#### Solution @sec-problem-6.4\n\n```python\ndef fold_change(expression_1, expression_2)\n    try:\n        return expression_1 / expression_2\n    except ZeroDivisionError:\n        print(\"expression_2 was zero!\")\n        return None\n```\n\n#### Solution @sec-problem-6.5\n\n```python\nclass SequenceLengthError(Exception):\n    pass\n\nMIN_LENGTH = 50\nMAX_LENGTH = 150\n\ndef validate_sequence_length(sequence):\n    sequence_length = len(sequence)\n\n    if sequence_length < MIN_LENGTH:\n        raise SequenceLengthError(f\"sequence length {sequence_length} was too short!\")\n\n    if sequence_length > MAX_LENGTH:\n        raise SequenceLengthError(f\"sequence length {sequence_length} was too long!\")\n\n    return None\n```\n\n#### Solution @sec-problem-6.6\n\n```python\ndef run_simulation(max_turns):\n    if max_turns < 1:\n        raise ValueError(f\"Expected at least 1 iteration, but got {max_turns=}\")\n\n    if max_turns > 1000:\n        raise ValueError(f\"Expected at most 1000 iterations, but got {max_turns=}\")\n\n    # Simulation code would follow\n    pass\n```\n\n## Chapter 7: Intro to Exploratory Data Analysis with Python\n\n### Stop & Think\n\n@tip-07-why-summarize-data response:  Looking at a summary of your data has a ton of benefits.  Summarizing your data can help you:\n\n- Identify data import issues, like importing the wrong file\n- Identify data quality issues, like missing values or outliers\n- Become familiar with the data, including data types and ranges\n\nThe more familiar you are with your data, the better!\n\n@tip-07-why-nice-plots response: Clean visualizations aren’t just important for presenting data to others, they are also important for presenting data to yourself!  A messy graph may hide trends, overload the viewer, or be otherwise difficult to interpret, potentially leading to misinterpretations or missed patterns.  Professional-looking visualizations can also be shared with colleagues through presentations and reports with minimal modifications, freeing up time later.  Lastly, it’s never bad to get in the habit of producing clear and effective visualizations of your data!\n\n@tip-07-good-color-palette response: Choosing a good color palette is critical for communicating information about your data.  Improper color choice can easily lead to misinterpretations.  In the heatmap example, we use a diverging color scheme to show the difference between positive and negative correlations, but the balance of those colors must be correct:\n\n- If the center of the palette was on 0.2 rather than zero, it would visually suggest that values around 0.2 are \"neutral\" or \"average,” rather than “positive”\n- With asymmetric color intensity (brightest blue at -0.2, brightest orange at 1.0), it would create visual bias, making positive correlations appear weaker than negative ones of the same magnitude\n\nIn both cases, viewers would easily be thrown off by the color scheme, even with a legend available.\n\n@tip-07-why-filter-columns response: Filtering columns by category can help manage cognitive load and distraction when working with large datasets, allowing you to focus on particular variables. It can make it easier to identify patterns, facilitate more targeted analysis, and help build intuition about how different aspects of the data relate to each other.\n\n@tip-07-what-correlation-tells-us response: Correlation values can show potential relationships between variables and suggest further avenues for investigation. However, they have limitations: they only measure linear relationships, can be heavily influenced by outliers, and don't indicate causation. They should be considered as starting points for deeper analysis rather than conclusive findings.\n\n@tip-07-log-scale-purpose response: While we are most used to linear scales, logarithmic scales are useful in many situations:\n\n⁃ When data spans several orders of magnitude, log scales lett you visualize both small and large values effectively on the same plot\n⁃ When data follows exponential growth patterns, like in the early stages of an [epidemic](https://data.europa.eu/apps/data-visualisation-guide/logarithmic-y-axes)\n⁃ When looking for proportional or percentage changes rather than count changes, like in the case of this [library example](https://www.lrs.org/2020/06/17/visualizing-data-the-logarithmic-scale/)\n\nIn fact, visualizing data on a logarithmic scale can sometimes reveal trends that are not apparent from count data, as in [this example using cancer data](https://www.pa.gov/content/dam/copapwp-pagov/en/health/documents/topics/healthstatistics/statistical-resources/understandinghealthstats/documents/Arithmetic_vs_Logarithmic_Line_Charts.pdf).\n\nLogarithmic transformations also have some nifty statistical applications, like [reducing skew in data](https://pmc.ncbi.nlm.nih.gov/articles/PMC9036143/).\n\n@tip-07-why-correlation-heatmap response: Heatmaps add an extra visual layer that is not present in tables (color), making strong correlations visually obvious.  Clustering reveals groups of variables that behave similarly, and the dendrograms show how those variables are related to each other.  Together, these components make patterns easier to spot than a table alone and add hierarchical structure that can’t be easily represented in a data table.\n\n@tip-07-merge-considerations response: When merging datasets, it’s important to consider:\n\n1. Whether incomplete data is acceptable or problematic.  If problematic, choose an inner join.\n2. If either dataset is primary to your analysis.  If no, choose an outer join.  If yes, choose a left or right join according to the dataset.\n\nAlong the way, consider how representative the resulting dataset will be and whether you will be able to use it effectively for any planned analyses or visualizations.\n\n### Practice Problems\n\n#### Solution @sec-problem-7.1\n\n```python\ndf = pd.DataFrame(state_cancer_data)\n```\n\n#### Solution @sec-problem-7.2\n\n```python\ndf[\"Cancer Deaths Per 100k\"] = df[\"Cancer Deaths\"] / df[\"Population\"] * 100_000\n```\n\n#### Solution @sec-problem-7.3\n\n```python\ndf.query(\"`Cancer Deaths Per 100k` >= 180 and `Median Household Income` < 68_500\")\n```\n\n#### Solution @sec-problem-7.4\n\n```python\ndf.plot(kind=\"scatter\", x=\"Median Household Income\", y=\"Cancer Deaths Per 100k\")\n```\n\n#### Solution @sec-problem-7.5\n\n```python\ndf.plot(kind=\"scatter\", x=\"Percent Aged 65+\", y=\"Cancer Deaths Per 100k\")\n```\n\n",
    "supporting": [
      "A01_solutions_files/figure-html"
    ],
    "filters": [],
    "includes": {}
  }
}