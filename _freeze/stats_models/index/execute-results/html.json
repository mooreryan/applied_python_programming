{
  "hash": "68bf9a8f1a1399a6e398b50dc7a80122",
  "result": {
    "engine": "jupyter",
    "markdown": "---\nauthor: \"Ryan M. Moore, PhD\"\ndate-modified: last-modified\ndate: \"2025-04-09\"\njupyter: python3\n---\n\n# Intro to Statistics & Modeling with Python {#sec-stats-models}\n\n## Introduction\n\nPython has become a cornerstone tool for statistical analysis in life sciences, offering powerful yet accessible ways to understand biological data. Statistical analysis helps scientists move beyond simple observations to make reliable inferences about experimental results and natural phenomena. Python's data science libraries (like NumPy, Pandas, and SciPy) provide ready-to-use functions that handle complex statistical calculations without requiring advanced mathematical knowledge. Modeling biological processes mathematically allows scientists to test hypotheses, predict outcomes, and discover patterns that might not be visible through direct observation. For life scientists, building statistical literacy is as important as lab techniques--it's how we determine if results are meaningful or merely coincidental. Python's visualization capabilities (through libraries like Matplotlib and Seaborn) help make abstract statistical concepts more concrete and interpretable. Understanding basic statistics in Python empowers biologists to design better experiments, analyze results more critically, and communicate findings more effectively.\n\n## Setup\n\nEnsure that you have the following packages installed:\n\n- [Matplotlib](https://matplotlib.org/)\n- [NumPy](https://numpy.org/)\n- [Pandas](https://pandas.pydata.org/)\n- [SciPy](https://scipy.org/)\n- [Seaborn](https://seaborn.pydata.org/)\n- [statsmodels](https://www.statsmodels.org/dev/index.html)\n\nNext, import all the required items for the chapter. We will use the common abbreviations when appropriate.\n\n::: {#ed30b367 .cell execution_count=1}\n``` {.python .cell-code}\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport statsmodels.formula.api\nimport statsmodels.stats.multicomp\n\nfrom collections import Counter\nfrom scipy import cluster\nfrom scipy import stats\nfrom statsmodels.multivariate.pca import PCA\n\npd.set_option(\"display.max_rows\", 10)\n```\n:::\n\n\n## Statistical Tests\n\nStatistical tests are mathematical methods that help researchers determine whether patterns in their data represent genuine relationships or simply random chance.\n\n_Note: Statistical tests all have caveats and other considerations that you need to be aware of before running the test and while interpreting the output. I suggest that you carefully read the docs for each of these functions as well as consult any relevant statistical literature when designing and analyzing your experiments._\n\n### Hypothesis Testing and p-values\n\nThe goal of a statistical test is to determine whether a specific hypothesis about data is likely true. When conducting such a test, we set up two hypotheses: the null hypothesis (H0) and the alternative hypothesis (HA). The null hypothesis typically states that there's no effect or relationship in our data (essentially, \"nothing interesting is happening\"), while the alternative hypothesis suggests that some effect or relationship does exist. If our test results show statistical significance, we reject the null hypothesis in favor of the alternative. However, if we don't find significance, we fail to reject the null hypothesis (rather than \"accepting\" it, a subtle but important distinction). To determine significance, we calculate a p-value, which tells us the probability of observing our test results (or something more extreme) if the null hypothesis were actually true. Before conducting the test, we choose a significance level (commonly 0.05) as our threshold. This means we're comfortable with a 5% risk of rejecting a null hypothesis that is actually true (known as a Type I error or false positive). On the flip side, we might also fail to reject a null hypothesis that's actually false. This is called a Type II error, and it happens when our test misses a real effect that exists in the data (false negative).\n\nFor more on hypothesis testing and p-values, see [Introduction to Hypothesis Testing](https://online.stat.psu.edu/stat500/lesson/6a/6a.1) and [Steps for Hypothesis Tests](https://online.stat.psu.edu/stat500/lesson/6a/6a.2).\n\nThis is probably a good time to mention that p-values have been controversial for quite a while now, for examples, see:\n\n- [Moving to a World Beyond \"_p_ < 0.05\"](https://doi.org/10.1080/00031305.2019.1583913)\n- [The ongoing tyranny of statistical significance testing in biomedical research](https://doi.org/10.1007/s10654-010-9440-x)\n- [A Dirty Dozen: Twelve P-Value Misconceptions](https://doi.org/10.1053/j.seminhematol.2008.04.003)\n\nThere are _many_ more examples to be found in the literature about this topic!\n\n_Note: For this document, we will use a p-value cutoff of 0.05, and talk about significance and rejecting null hypotheses in as basic a way as possible, but you should keep the above criticism in mind._\n\n::: {#tip-08-pvalue-misconceptions .callout-tip title=\"Stop & Think\" collapse=\"false\"}\nWhat's one potential problem with relying solely on p-values when interpreting experimental results?\n:::\n\n### Brief Glossary\n\nWe're going to touch briefly on some common statistical tests. Before we do, let's review a few terms:\n\n- **Normal distribution:** Data is [normally distributed](https://www.khanacademy.org/math/statistics-probability/modeling-distributions-of-data/normal-distributions-library/a/normal-distributions-review) if it fits a symmetrical bell curve and the mean and median are both at the center of the distribution.\n- **Independent sample:** Sample groups are independent if observations do not influence each other.\n- **Paired sample:** Samples are paired (or dependent) when there is a meaningful relationship between observations in two or more groups. These can be measurements of the same individual over time or related measurements of two or more individuals. There are good examples of when this might happen [here](https://online.stat.psu.edu/stat200/lesson/1/1.4/1.4.3) and [here](https://www.statisticssolutions.com/free-resources/directory-of-statistical-analyses/paired-sample-t-test/).\n- **Assumption:** All statistical tests assume things about your data in their calculations. The big ones are usual the [distribution, independence, skewness, and linearity](https://real-statistics.com/descriptive-statistics/assumptions-statistical-test/). Performing statistical tests on data that does not meet the correct assumptions can lead to erroneous results, though this may vary with sample size. Statistical tests generally fall into two categories (see the articles [Parametric and Nonparametric: Demystifying the Terms](https://www.mayo.edu/research/documents/parametric-and-nonparametric-demystifying-the-terms/doc-20408960) and [Nonparametric statistical tests: friend or foe?](https://doi.org/10.36416/1806-3756/e20210292) for more thorough explanations and comparisons):\n  - **Parametric tests:** These assume specific data characteristics, generally a normal distribution. These assumptions matter most with smaller sample sizes.\n  - **Nonparametric tests:** These make fewer assumptions about your data's shape and other parameters. They often work by analyzing ranks rather than raw values.\n\n### Comparing Two Groups\n\nThe statistical comparison of two groups is one of the more common operations that you will need to perform. This covers everything from comparing gene expression levels between healthy and diseased tissue to measuring enzyme activity before and after drug treatment. Comparing two groups is mostly done with the venerable t test, as well as the Mann-Whitney and Wilcoxon tests when the underlying data is not sampled from a normal distribution.\n\nOne distinction that needs to be made is in the experimental design, that is, whether your groups are paired or not. Paired groups are measurements taken from the same subjects under different conditions (like before and after treatment). Alternatively, unpaired groups are measurements from different subjects (like control group versus experimental group).\n\nWe will start with unpaired groups, since dealing with them is a bit simpler.\n\n#### Unpaired Groups\n\nComparing two unpaired groups is a classic problem, and you have probably needed this operation many times in your own research. Let's start by looking at the unpaired [t test](https://www.jmp.com/en/statistics-knowledge-portal/t-test).\n\n##### Unpaired t Test\n\nAn unpaired t test, also known as a [two-sample t test](https://www.jmp.com/en/statistics-knowledge-portal/t-test/two-sample-t-test), compares means between two independent groups. This is the classic t test that you may remember from your statistics courses. Let's create some example data and visualize it to see how it works.\n\n::: {#f19c676d .cell execution_count=2}\n``` {.python .cell-code}\n# Ensures the same random numbers are generated each time.\nnp.random.seed(9382741)\n\n# Create a DataFrame with two columns of random data:\ndf = pd.DataFrame(\n    {\n        # \"Group A\": Generate 40 random values from a normal distribution\n        # with mean (loc) of 5 and standard deviation (scale) of 3\n        \"Group A\": stats.norm(loc=5, scale=3).rvs(size=40),\n        # \"Group B\": Generate 40 random values from a normal distribution\n        # with mean (loc) of 8 and standard deviation (scale) of 3\n        \"Group B\": stats.norm(loc=8, scale=3).rvs(size=40),\n    }\n)\n\n# Create a categorical strip plot (jittered points) using seaborn\n# - 'kind=\"strip\"' specifies we want individual data points with jitter\n# - 'height=3' sets the height of the plot in inches\n# - 'aspect=1.6' sets the width-to-height ratio\nsns.catplot(df, kind=\"strip\", height=3, aspect=1.6)\n\n# Overlay a boxplot on the same axes\n# - 'width=0.25' makes the boxes narrower than default\n# - 'fill=False' creates unfilled boxes (just outlines)\n# - 'color=\"#555555\"' sets the box color to a dark gray\nsns.boxplot(df, width=0.25, fill=False, color=\"#555555\")\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-3-output-1.png){}\n:::\n:::\n\n\nNow that we have seen the data, let's run the t test. To do it, we use the [ttest_ind()](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.ttest_ind.html#scipy.stats.ttest_ind) function from [SciPy](https://docs.scipy.org/doc/scipy/index.html).\n\n::: {#3de37594 .cell execution_count=3}\n``` {.python .cell-code}\n# Perform an unpaired t test comparing Group A and Group B\n# stats.ttest_ind() performs the independent samples t test\n# df[\"Group A\"] and df[\"Group B\"] are the two samples we're comparing\n# .pvalue extracts just the p-value from the test results\nresult = stats.ttest_ind(df[\"Group A\"], df[\"Group B\"]).pvalue\n\n# Calculate the mean value for each column in the data frame\n# - df.mean() calculates the mean of each column in the original data frame\n# - columns=[\"Mean\"] sets the name of the resulting column to \"Mean\"\nmeans = pd.DataFrame(df.mean(), columns=[\"Mean\"])\n\ndisplay(means)\nprint(f\"Unpaired t test p-value: {result:.1e}\")\n```\n\n::: {.cell-output .cell-output-display}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Mean</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>Group A</th>\n      <td>5.345036</td>\n    </tr>\n    <tr>\n      <th>Group B</th>\n      <td>7.455855</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nUnpaired t test p-value: 1.5e-03\n```\n:::\n:::\n\n\nThe p-value of the t test is 1.5e-03 so we reject the null hypothesis that the group means are the same.\n\nThe t test comes with several important assumptions that should be met for the results to be valid:\n\n- Samples are independent\n- Samples have equal variance (homoscedasticity or equal spread of data)\n- Data follows a normal distribution\n- Random sampling from the population\n\n##### Mann-Whitney Test\n\nIf your data doesn't follow a normal distribution, you may need a nonparametric alternative like the [Mann-Whitney Test](https://www.statisticssolutions.com/free-resources/directory-of-statistical-analyses/mann-whitney-u-test/). Instead of comparing means, this test determines if values from one group tend to be larger than values from another group.\n\nFirst, we generate and visualize some data. We will generate some data from a uniform distribution (non-normal), and each of the samples will be small (10 observations each). This is a pretty good use-case for the Mann-Whitney test.\n\n::: {#tip-08-parametric-vs-nonparametric .callout-tip title=\"Stop & Think\" collapse=\"false\"}\nWhen might you choose a nonparametric test like Mann-Whitney over a parametric test like the t test?\n:::\n\n::: {#5782730c .cell execution_count=4}\n``` {.python .cell-code}\nnp.random.seed(9382741)\ndf = pd.DataFrame(\n    {\n        \"Group A\": stats.uniform(loc=5, scale=10).rvs(size=6),\n        \"Group B\": stats.uniform(loc=8, scale=10).rvs(size=6),\n    }\n)\n\nsns.catplot(df, kind=\"strip\", height=3, aspect=1.6)\nsns.boxplot(df, width=0.25, fill=False, color=\"#555555\")\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-5-output-1.png){}\n:::\n:::\n\n\nJust from looking at the data, it appears that there might be a real difference between the two groups. Let's run the Mann-Whitney test to check if they are significantly different.\n\n::: {#b3f172cf .cell execution_count=5}\n``` {.python .cell-code}\n# Perform the Mann-Whitney U test comparing Groups A and B\n# (We extract just the p-value from the test results with the `.pvalue` at the end)\nresult = stats.mannwhitneyu(df[\"Group A\"], df[\"Group B\"]).pvalue\n\n# Calculate the median value for each column in the data frame\nmedians = pd.DataFrame(df.median(), columns=[\"Median\"])\n\ndisplay(medians)\nprint(f\"Mann-Whitney test p-value: {result:.2f}\")\n```\n\n::: {.cell-output .cell-output-display}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Median</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>Group A</th>\n      <td>8.414715</td>\n    </tr>\n    <tr>\n      <th>Group B</th>\n      <td>12.917969</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nMann-Whitney test p-value: 0.03\n```\n:::\n:::\n\n\nCool! So we see that the Mann-Whitney test returned a p-value < 0.05 so we reject the null hypothesis that the distribution underlying group A is the same as the distribution underlying group B.\n\nWhile the Mann-Whitney test doesn't assume that the samples come from a normal distribution, it does still have assumptions that should be met and that can affect your interpretation of the results. For more info see [here](https://www.statisticssolutions.com/free-resources/directory-of-statistical-analyses/mann-whitney-u-test/).\n\n#### Paired Groups\n\n- For comparing the means of paired samples, you will want to use a [paired t test](https://www.jmp.com/en/statistics-knowledge-portal/t-test/paired-t-test).\n- The nonparametric companion to the paired t test is the [Wilcoxon test](https://datatab.net/tutorial/wilcoxon-test).\n\n##### Paired t test\n\nWe use the [scipy.stats.ttest_rel()](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.ttest_rel.html#scipy.stats.ttest_rel) function from SciPy to perform a paired t-test. This statistical test examines whether two related samples (like before-and-after measurements from the same subjects) have the same average values. The test evaluates the null hypothesis that the means of the two paired groups are identical.\n\nSame as before, let's create some data and make some box plots to check it out. In this dataset, we would expect to see about a 2 unit increase in the post treatment measurements.\n\n::: {#fb1ea202 .cell execution_count=6}\n``` {.python .cell-code}\n# Set the random seed for reproducibility\nnp.random.seed(964832)\n\n# Generate random pre-treatment measurements from a normal distribution\n# with mean=50 and standard deviation=10 for 40 subjects\nbefore_treatment = stats.norm(loc=50, scale=10).rvs(size=40)\n\n# Create a DataFrame with two columns:\n# - 'Pre': the pre-treatment measurements\n# - 'Post': the pre-treatment values plus a small random effect\n#   (representing treatment effect) from a normal distribution\n#   with mean=2 and standard deviation=1\ndf = pd.DataFrame(\n    {\n        \"Pre\": before_treatment,\n        \"Post\": before_treatment + stats.norm(loc=2, scale=1).rvs(size=40),\n    }\n)\n\n# Create a strip plot (jittered points) to visualize the individual data points\n# in both Pre and Post groups\nsns.catplot(df, kind=\"strip\", height=3, aspect=1.6)\n\n# Add box plots over the strip plots to show summary statistics\n# width=0.25 makes the boxes narrow\n# fill=False means the boxes are transparent\n# color=\"#555555\" sets the box color to a dark gray\nsns.boxplot(df, width=0.25, fill=False, color=\"#555555\")\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-7-output-1.png){}\n:::\n:::\n\n\nNext, we run the paired t test.\n\n::: {#31e34107 .cell execution_count=7}\n``` {.python .cell-code}\n# Run the t test\nresult = stats.ttest_rel(df[\"Pre\"], df[\"Post\"]).pvalue\n\n# This is to make a nice table with the means.\nmeans = pd.DataFrame(df.mean(), columns=[\"Mean\"])\n\ndisplay(means)\nprint(f\"Unpaired t test p-value: {result:.1e}\")\n```\n\n::: {.cell-output .cell-output-display}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Mean</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>Pre</th>\n      <td>50.150032</td>\n    </tr>\n    <tr>\n      <th>Post</th>\n      <td>52.008654</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nUnpaired t test p-value: 3.3e-12\n```\n:::\n:::\n\n\nIt is statistically significant! But wait, while our p-value confirms this, we need to consider whether 2 units (whatever those units represent in your experiment) actually matters in your biological system. This is where your expertise as a researcher becomes crucial. It's important not to rely solely on p-values when interpreting results. In our example, we had sufficient sample size to detect a difference, but that difference might still be too small to have any meaningful biological impact.\n\n::: {#tip-08-statistical-vs-biological .callout-tip title=\"Stop & Think\" collapse=\"false\"}\nWhy is it important to consider both statistical significance and biological relevance when interpreting results?\n:::\n\nAs you can see, this analysis is quite similar to the unpaired test we discussed earlier. The main difference to keep in mind is how you interpret the results: with paired tests, you're looking at changes within the same subjects (like before and after treatment), rather than differences between separate groups.\n\n###### Aside: Effect Size and p-values\n\nScientists have a tendency to focus strongly on p-values, but there is another measure that arguably deserves more attention: _effect size_. Effect size tells us something different than a p-value: how large or meaningful that effect actually is. It measures the magnitude of differences between groups or the strength of relationships between variables in practical terms. In studies with large sample sizes, it's very common to find statistically significant results (tiny p-values) alongside tiny effect sizes. This creates a situation where a difference might be technically \"real\" but practically meaningless. This is precisely why researchers often report both p-values and effect sizes together. P-values answer \"Is there an effect?\", while effect sizes answer the equally crucial question: \"Does this effect actually matter?\" Together, they provide a much more complete and actionable understanding of research findings.\n\nFor more on effect sizes, see [Using Effect Size--or Why the _P_ Value Is Not Enough](https://doi.org/10.4300/JGME-D-12-00156.1).\n\n::: {#tip-08-effect-size-examples .callout-tip title=\"Stop & Think\" collapse=\"false\"}\nCan you think of a scenario in the life sciences where a statistically significant result might have a negligible effect size?\n:::\n\n##### Wilcoxon test\n\nThe Wilcoxon signed-rank test is used to test the null hypothesis that two related paired samples come from the same distribution. Here is what the SciPy docs have to say about the [Wilcoxon signed-rank test](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.wilcoxon.html#scipy.stats.wilcoxon):\n\n> The Wilcoxon signed-rank test tests the null hypothesis that two related paired samples come from the same distribution. In particular, it tests whether the distribution of the differences x - y is symmetric about zero. It is a non-parametric version of the paired t test.\n\n_Note: this is pretty similar to the Mann-Whitney test...we will leave the distinction to your stats professors :)_\n\nAgain, let's make some fake data and see how it looks.\n\n::: {#6f5a567f .cell execution_count=8}\n``` {.python .cell-code}\n# Set a random seed for reproducibility\nnp.random.seed(9382741)\n\n# Create a DataFrame with two columns:\n# 1. \"Pre-Treatment\": 6 random values from a uniform distribution\n#    between 5 and 11 (loc=5, scale=6)\n# 2. \"Post-Treatment\": 6 random values from a uniform distribution\n#    between 6 and 12 (loc=6, scale=6)\ndf = pd.DataFrame(\n    {\n        \"Pre-Treatment\": stats.uniform(loc=5, scale=6).rvs(size=6),\n        \"Post-Treatment\": stats.uniform(loc=6, scale=6).rvs(size=6),\n    }\n)\n\n# Create a strip plot (individual data points) using Seaborn's catplot function\n# - height=3 and aspect=1.6 control the dimensions of the plot\nsns.catplot(df, kind=\"strip\", height=3, aspect=1.6)\n\n# Add a boxplot on top of the strip plot to show distribution statistics\n# - width=0.25: makes the boxes narrower than default\n# - fill=False: creates transparent boxes so we can still see the strip plot points\n# - color=\"#555555\": sets the box and whisker color to a dark gray\nsns.boxplot(df, width=0.25, fill=False, color=\"#555555\")\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-9-output-1.png){}\n:::\n:::\n\n\nLooks like a bit of difference there, but not too strong. Time for a statistical test!\n\n::: {#d7866873 .cell execution_count=9}\n``` {.python .cell-code}\n# Run the Wilcoxon signed-rank test\nresult = stats.wilcoxon(df[\"Pre-Treatment\"], df[\"Post-Treatment\"]).pvalue\n\n# This is to make a nice table with the medians.\nmedians = pd.DataFrame(df.median(), columns=[\"Median\"])\n\ndisplay(medians)\nprint(f\"Mann-Whitney test p-value: {result:.2f}\")\n```\n\n::: {.cell-output .cell-output-display}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Median</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>Pre-Treatment</th>\n      <td>7.048829</td>\n    </tr>\n    <tr>\n      <th>Post-Treatment</th>\n      <td>8.950781</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nMann-Whitney test p-value: 0.16\n```\n:::\n:::\n\n\nIn this case we fail to reject the null hypothesis that the paired samples com from the same distribution (i.e., that the difference between them is symmetric about zero).\n\nSimilar to the Mann-Whitney test, though the Wilcoxon signed-rank test doesn't assume your data comes from a normal population it does s other assumptions to be aware of. Always consult the relevant statistical literature prior to using these tests in your research.\n\n### Comparing Multiple Groups\n\nIn the previous sections, we have been focused on comparing two groups. You will often run into cases in which you have more than two groups, or other experimental designs which preclude you from using more straightforward techniques. Let's see how we can deal with that now.\n\nTo compare multiple groups, we most commonly use an Analysis of Variance (ANOVA) test. The [one-way ANOVA](https://www.jmp.com/en/statistics-knowledge-portal/one-way-anova) tests the null hypothesis that two or more groups have the same population mean. It's nonparametric partner is the [Kruskal-Wallis test](https://library.virginia.edu/data/articles/getting-started-with-the-kruskal-wallis-test), which uses ranks instead of raw values, and tests the null hypothesis that the population median of all of the groups are equal.\n\nLet's revisit the cancer example from last chapter @sec-eda. For the purposes here, we will generate some fake numbers for deaths per 100k people for each of the [geographic regions](https://www.bls.gov/regions/home.htm) specified by the U.S. Bureau of Labor Statistics.\n\nImagine we have data points for three locations per state within each region (just assume these are some relevant locations for public health: counties, metro areas, whatever). In this example, we are counting the District of Columbia as a state.\n\nLet's write a function to generate the data:\n\n::: {#26e54a03 .cell execution_count=10}\n``` {.python .cell-code}\n# Dictionary mapping each U.S. region to the number of states it contains\nREGIONS = {\n    \"Northeast\": 7,\n    \"Mid-Atlantic\": 7,\n    \"Southeast\": 8,\n    \"Midwest\": 10,\n    \"Mountain-Plains\": 6,\n    \"Southwest\": 5,\n    \"West\": 8,\n}\n\n# Number of sampling locations we'll collect data from in each state\nSAMPLING_LOCATIONS_PER_STATE = 3\n\n\ndef generate_cancer_data(region, mean, sd):\n    \"\"\"\n    Generate simulated cancer death data for a specific region.\n\n    Parameters:\n        region (str): The name of the region from the REGIONS dictionary\n        mean (float): The mean cancer death count for this region\n        sd (float): The standard deviation of cancer deaths for this region\n\n    Returns:\n        list: A list of dictionaries containing region and death count data\n    \"\"\"\n    # Calculate how many states are in this region\n    state_count = REGIONS[region]\n\n    # Calculate total number of observations needed\n    # (states in region * sampling locations per state)\n    observation_count = state_count * SAMPLING_LOCATIONS_PER_STATE\n\n    # Generate random cancer death counts from a normal distribution\n    # with the specified mean and standard deviation\n    observations = stats.norm(loc=mean, scale=sd).rvs(observation_count)\n\n    # Convert the raw numbers into a list of dictionaries with region labels\n    result = [{\"Region\": region, \"Deaths\": observation} for observation in observations]\n\n    return result\n\n\n# Create a pandas DataFrame by combining cancer data from all regions\n# Each region is assigned different mean death rates to simulate regional variations\n# The standard deviation is kept constant at 20 across all regions\ncancer = pd.DataFrame(\n    generate_cancer_data(\"Northeast\", mean=200, sd=20)\n    + generate_cancer_data(\"Mid-Atlantic\", mean=200, sd=20)\n    + generate_cancer_data(\"Southeast\", mean=225, sd=20)  # Highest mean death rate\n    + generate_cancer_data(\"Midwest\", mean=195, sd=20)\n    + generate_cancer_data(\"Mountain-Plains\", mean=175, sd=20)\n    + generate_cancer_data(\"Southwest\", mean=175, sd=20)\n    + generate_cancer_data(\"West\", mean=150, sd=20)  # Lowest mean death rate\n)\n\n# Display the resulting DataFrame\ncancer\n```\n\n::: {.cell-output .cell-output-display execution_count=10}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Region</th>\n      <th>Deaths</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Northeast</td>\n      <td>232.569070</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Northeast</td>\n      <td>171.141188</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Northeast</td>\n      <td>196.646856</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Northeast</td>\n      <td>185.546208</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Northeast</td>\n      <td>215.395855</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>148</th>\n      <td>West</td>\n      <td>187.082183</td>\n    </tr>\n    <tr>\n      <th>149</th>\n      <td>West</td>\n      <td>164.184395</td>\n    </tr>\n    <tr>\n      <th>150</th>\n      <td>West</td>\n      <td>123.225502</td>\n    </tr>\n    <tr>\n      <th>151</th>\n      <td>West</td>\n      <td>199.988325</td>\n    </tr>\n    <tr>\n      <th>152</th>\n      <td>West</td>\n      <td>159.141206</td>\n    </tr>\n  </tbody>\n</table>\n<p>153 rows × 2 columns</p>\n</div>\n```\n:::\n:::\n\n\nCheck out the resulting data table. We generate the data to look like this because it is a pretty good representation of how real data might be structured in a CSV or other document. Now let's plot it:\n\n::: {#304bf085 .cell execution_count=11}\n``` {.python .cell-code}\nsns.catplot(cancer, x=\"Deaths\", y=\"Region\", hue=\"Region\", kind=\"box\")\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-12-output-1.png){}\n:::\n:::\n\n\nLooks like there may be some significant differences there. Let's run some statistical tests to see if we can assume that some of the means are different from the others.\n\n##### One-Way ANOVA\n\nWe can use SciPy's [f_oneway()](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.f_oneway.html#scipy.stats.f_oneway) function to run the one-way ANOVA. Here is the blurb from the docs about `f_oneway()`:\n\n> The one-way ANOVA tests the null hypothesis that two or more groups have the same population mean. The test is applied to samples from two or more groups, possibly with differing sizes.\n\nThe `f_oneway()` function doesn't really want to work with data in the format we have it, so we have to pull out each group and pass it in separately:\n\n::: {#5139ca49 .cell execution_count=12}\n``` {.python .cell-code}\n# Create a list of Series objects, where each Series contains the 'Deaths' values\n# for a specific region from the cancer dataset\ndeaths = [cancer.query(f\"Region == '{region}'\")[\"Deaths\"] for region in REGIONS.keys()]\n\n# Perform a one-way ANOVA test using the deaths data from all regions\n# - The * operator unpacks the deaths list so each Series becomes a separate argument\n# - stats.f_oneway compares means of two or more independent samples to determine if they're significantly different\nresult = stats.f_oneway(*deaths)\n\n# Print the results of the ANOVA test\n# - The :.1f formats the F statistic to 1 decimal place\n# - The :.1e formats the p-value in scientific notation with 1 decimal place\nprint(f\"F stat: {result.statistic:.1f}, p-value: {result.pvalue:.1e}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nF stat: 21.4, p-value: 6.2e-18\n```\n:::\n:::\n\n\nThat's a significant value, so we reject the null hypothesis that the groups have the same population mean. (Like all the other tests mentioned, it's a good idea to make sure you are familiar with any assumptions of the test, to ensure you are using and interpreting the results correctly.)\n\nJust for fun, let's write some \"fancy\" code:\n\n::: {#1ca5db3b .cell execution_count=13}\n``` {.python .cell-code}\n# Reshape the cancer data frame from long to wide format, with regions as columns\ncancer_wide = cancer.pivot(columns=\"Region\")\n\n# Create a list of Series, each containing the cancer deaths for one region\n# The dropna() removes any missing values from each region's data\ndeaths_by_region = [cancer_wide[region].dropna() for region in cancer_wide.columns]\n\n# Perform a one-way ANOVA test to compare means across all regions\n# - The * operator unpacks the list so each region's data is passed as a separate argument\nresult = stats.f_oneway(*deaths_by_region)\n\n# Print the results, formatting F-statistic to 1 decimal place and p-value in scientific notation\nprint(f\"F stat: {result.statistic:.1f}, p-value: {result.pvalue:.1e}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nF stat: 21.4, p-value: 6.2e-18\n```\n:::\n:::\n\n\nPretty neat!\n\n::: {#tip-08-which-anova .callout-tip title=\"Stop & Think\" collapse=\"false\"}\nWhich version of the code do you like better? Which do you think is more readable?\n:::\n\n##### Kruskal-Wallis\n\nThe ANOVA's non-parametric partner is the [Kruskal-Wallis test](https://library.virginia.edu/data/articles/getting-started-with-the-kruskal-wallis-test). It uses ranks instead of raw values, and tests the null hypothesis that the population median of all of the groups are equal.\n\nThe SciPy function for running the Kruskal-Wallis test is called [kruskal()](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.kruskal.html#scipy.stats.kruskal). The blurb from the SciPy docs about the test is pretty informative:\n\n> The Kruskal-Wallis H-test tests the null hypothesis that the population median of all of the groups are equal. It is a non-parametric version of ANOVA. The test works on 2 or more independent samples, which may have different sizes. Note that rejecting the null hypothesis does not indicate which of the groups differs. Post hoc comparisons between groups are required to determine which groups are different.\n\nThe format is pretty much the same as for the ANOVA.\n\n::: {#f0f7788a .cell execution_count=14}\n``` {.python .cell-code}\nresult = stats.kruskal(*deaths)\nprint(f\"H stat: {result.statistic:.1f}, p-value: {result.pvalue:.1e}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nH stat: 72.5, p-value: 1.3e-13\n```\n:::\n:::\n\n\nThe p-value is less than our critical threshold of 0.05, so we reject the null hypothesis that median of all of the groups are equal. Just like with the ANOVA, it doesn't tell us which of the groups is different. We will tackle that question in the next section.\n\nI'm going to start to sound like a broken record here, but, like every other test, make sure you know the assumptions and are using the test correctly if you are using it in your own research!\n\n#### Post-Hoc Testing\n\nAn ANOVA or Kruskal-Wallis test will tell you if there is a significant difference somewhere among multiple groups, but they don't say which specific groups differ from each other. For this, we need to perform a follow up or [_post-hoc test_](https://statisticsbyjim.com/anova/post-hoc-tests-anova/). [Tukey's Honestly Significant Difference (Tukey's HSD)](https://www.statisticshowto.com/probability-and-statistics/statistics-definitions/post-hoc/tukey-test-honest-significant-difference/) is one of the most popular post-hoc methods for ANOVA ([Dunn's test](https://journals.sagepub.com/doi/pdf/10.1177/1536867X1501500117), which we won't cover in this chapter, is the most popular for Kruskal-Wallis). It systematically compares all pairs of groups and adjusts the resulting p-values (more on this in the next section). By using a post-hoc test, we can identify which groups differ significantly from each other, transforming a general \"something's different\" conclusion into specific insights about where those differences lie.\n\nWe can run Tukey's HSD using the [tukey_hsd()](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.tukey_hsd.html#scipy.stats.tukey_hsd) from the SciPy package. Here is SciPy's docs have to say about the function:\n\n> The null hypothesis is that the distributions underlying the samples all have the same mean. The test statistic, which is computed for every possible pairing of samples, is simply the difference between the sample means. For each pair, the p-value is the probability under the null hypothesis (and other assumptions; see notes) of observing such an extreme value of the statistic, considering that many pairwise comparisons are being performed. Confidence intervals for the difference between each pair of means are also available.\n\n_Note: See how they mention being aware of the assumptions of the test? Make sure to check those out before using Tukey's HSD in your own research!_\n\n##### Tukey's HSD with SciPy\n\nYou call the `tukey_hsd()` function just like the `f_oneway()` and `kruskal()` functions. We will use the same data again:\n\n::: {#324d2741 .cell execution_count=15}\n``` {.python .cell-code}\nresult = stats.tukey_hsd(*deaths)\nprint(result)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nTukey's HSD Pairwise Group Comparisons (95.0% Confidence Interval)\nComparison  Statistic  p-value  Lower CI  Upper CI\n (0 - 1)      2.357     1.000   -16.875    21.588\n (0 - 2)    -16.746     0.108   -35.367     1.875\n (0 - 3)      1.341     1.000   -16.390    19.071\n (0 - 4)     28.182     0.001     8.165    48.199\n (0 - 5)     26.825     0.004     5.757    47.892\n (0 - 6)     40.853     0.000    22.232    59.474\n (1 - 0)     -2.357     1.000   -21.588    16.875\n (1 - 2)    -19.103     0.040   -37.724    -0.482\n (1 - 3)     -1.016     1.000   -18.747    16.715\n (1 - 4)     25.825     0.003     5.808    45.842\n (1 - 5)     24.468     0.012     3.401    45.535\n (1 - 6)     38.496     0.000    19.875    57.117\n (2 - 0)     16.746     0.108    -1.875    35.367\n (2 - 1)     19.103     0.040     0.482    37.724\n (2 - 3)     18.087     0.030     1.021    35.153\n (2 - 4)     44.928     0.000    25.497    64.359\n (2 - 5)     43.571     0.000    23.060    64.082\n (2 - 6)     57.599     0.000    39.609    75.588\n (3 - 0)     -1.341     1.000   -19.071    16.390\n (3 - 1)      1.016     1.000   -16.715    18.747\n (3 - 2)    -18.087     0.030   -35.153    -1.021\n (3 - 4)     26.841     0.001     8.261    45.420\n (3 - 5)     25.484     0.003     5.777    45.190\n (3 - 6)     39.512     0.000    22.446    56.578\n (4 - 0)    -28.182     0.001   -48.199    -8.165\n (4 - 1)    -25.825     0.003   -45.842    -5.808\n (4 - 2)    -44.928     0.000   -64.359   -25.497\n (4 - 3)    -26.841     0.001   -45.420    -8.261\n (4 - 5)     -1.357     1.000   -23.143    20.429\n (4 - 6)     12.671     0.451    -6.760    32.102\n (5 - 0)    -26.825     0.004   -47.892    -5.757\n (5 - 1)    -24.468     0.012   -45.535    -3.401\n (5 - 2)    -43.571     0.000   -64.082   -23.060\n (5 - 3)    -25.484     0.003   -45.190    -5.777\n (5 - 4)      1.357     1.000   -20.429    23.143\n (5 - 6)     14.028     0.391    -6.483    34.539\n (6 - 0)    -40.853     0.000   -59.474   -22.232\n (6 - 1)    -38.496     0.000   -57.117   -19.875\n (6 - 2)    -57.599     0.000   -75.588   -39.609\n (6 - 3)    -39.512     0.000   -56.578   -22.446\n (6 - 4)    -12.671     0.451   -32.102     6.760\n (6 - 5)    -14.028     0.391   -34.539     6.483\n\n```\n:::\n:::\n\n\nThe `tukey_hsd()` function returns an instance of the `TukeyHSDResult` class. You may find that the `TukeyHSDResult` class is not exactly the most user-friendly class. To get the data to put on a chart, you have to do a little bit of work:\n\n::: {#d86f7278 .cell execution_count=16}\n``` {.python .cell-code}\n# Perform Tukey's HSD (Honest Significant Difference) test on the death data\n# This test compares all possible pairs of regions to find significant differences\nresult = stats.tukey_hsd(*deaths)\n\n# Generate 95% confidence intervals for the differences between means\nconfidence_interval = result.confidence_interval(0.95)\n\n# Get the list of region names from the REGIONS dictionary\nregions = list(REGIONS.keys())\n\n# Create an empty list to store the results for each pair of regions\nrows = []\n\n# Loop through all pairs of regions using numpy's ndenumerate to get indices and p-values\nfor (i, j), p_value in np.ndenumerate(result.pvalue):\n    # Skip comparing a region to itself (i.e., the diagonal elements)\n    if i != j:\n        # Create a dictionary with all the information for this pair of regions\n        row = {\n            \"Region 1\": regions[i],\n            \"Region 2\": regions[j],\n            # Create a label for the pair\n            \"Pair\": regions[i] + \"-\" + regions[j],\n            # The mean difference between regions\n            \"Difference in Means\": result.statistic[i, j],\n            \"p-value\": p_value,\n            \"Significant?\": p_value < 0.05,\n            # Lower bound of confidence interval\n            \"CI Low\": confidence_interval.low[i, j],\n            # Upper bound of confidence interval\n            \"CI High\": confidence_interval.high[i, j],\n        }\n\n        # Add this pair's results to our list\n        rows.append(row)\n\n# Convert the list of dictionaries to a pandas DataFrame\n# Sort by the difference in means and reset the index\ntukey_df = pd.DataFrame(rows).sort_values(\"Difference in Means\").reset_index()\ntukey_df  # Display the DataFrame\n\n# Create a categorical plot using seaborn\n# This will show the difference in means for each pair of regions\nfacet_grid = sns.catplot(\n    tukey_df,\n    y=\"Pair\",\n    x=\"Difference in Means\",\n    hue=\"Significant?\",\n    height=7,\n    aspect=0.8,\n)\n\n# Get the current axis object to add more elements to the plot\n# - 'ax' is a common abbreviation you will see when using matplotlib\nax = facet_grid.facet_axis(0, 0)\n\n# Add confidence interval lines for each pair\nfor idx, row in tukey_df.iterrows():\n    # Plot a horizontal line for each confidence interval\n    ax.plot([row[\"CI Low\"], row[\"CI High\"]], [idx, idx], color=\"#333333\", linewidth=1)\n\n# Add a vertical line at x=0\n# This helps visualize which differences are positive vs. negative\n# If a confidence interval crosses this line, the difference is not significant\nax.axvline(0, linestyle=\"--\", color=\"#BBBBBB\")\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-17-output-1.png){}\n:::\n:::\n\n\n_Note: You may want to check out the docs for the [numpy.ndenumerate()](https://numpy.org/doc/2.2/reference/generated/numpy.ndenumerate.html) function as we haven't covered it before now._\n\nWe added in the confidence intervals to the plot. As you can see, the pairs whose confidence interval overlaps with zero are not-significantly different by the test, while those whose confidence interval does _not_ overlap with zero are significantly different by the test.\n\nThis code is a bit finicky, and we would like to avoid that sort of code whenever possible. Additionally, the plot itself is fairly tricky to read since there are so many pairs. To help with both these issues, let's pull in the [statsmodels](https://www.statsmodels.org/stable/index.html) package as it has a nice, built-in way to visualize Tukey's HSD results.\n\n##### Tukey's HSD with statsmodels\n\nTo run the test using the statsmodels package, we use [pairwise_tukeyhsd()](https://www.statsmodels.org/dev/generated/statsmodels.stats.multicomp.pairwise_tukeyhsd.html), and to generate the plot we use the [plot_simultaneous()](https://www.statsmodels.org/dev/generated/statsmodels.sandbox.stats.multicomp.TukeyHSDResults.plot_simultaneous.html#statsmodels.sandbox.stats.multicomp.TukeyHSDResults.plot_simultaneous) function. Let's see how easy it is to run:\n\n::: {#7b13d6bd .cell execution_count=17}\n``` {.python .cell-code}\nresult = statsmodels.stats.multicomp.pairwise_tukeyhsd(cancer[\"Deaths\"], groups=cancer[\"Region\"], alpha=0.05)\n# There is a small issue where if you don't set the result of this to a variable,\n# you get two charts instead of one.\nfigure = result.plot_simultaneous()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-18-output-1.png){}\n:::\n:::\n\n\nThat's pretty cool! Now, instead of looking at all those pairs, we can look at confidence intervals for each region. If they do not overlap with another region, then those two regions have significantly different deaths per 100k people. Not only did we get a plot that more effectively shows the results, we also got to chop out a ton of code!\n\n::: {#tip-08-post-hoc-interpretation .callout-tip title=\"Stop & Think\" collapse=\"false\"}\nLooking at the Tukey's HSD plot, how would you explain which regions have significantly different cancer death rates?\n:::\n\n## Linear Regression\n\nLinear regression is incredibly common, and I'm absolutely positive that you all have already used it in your research. Let's see how it goes in Python!\n\nLinear regression uses one or more explanatory (independent) variables to predict a response (dependent) variable. Simple linear regression refers to a model with only one explanatory variable, while models with two or more explanatory variables are multiple linear regression models. In both cases, we fit a linear model to the data, meaning that we are expecting a straight, non-vertical line. (There's a really snazzy visual guide to linear regression [here](https://mlu-explain.github.io/linear-regression/).)\n\nLet's generate and take a look at some data.\n\n::: {#558651fb .cell execution_count=18}\n``` {.python .cell-code}\n# This ensures that random numbers generated will be the same every time the\n# code runs\nnp.random.seed(254370)\n\n# Define how many data points we want to generate\nobservation_count = 100\n\n# Generate X values from a uniform distribution\n# - loc=-50: The lower bound of the distribution\n# - scale=100: The range of the distribution (from -50 to -50+100=50)\n# - size=observation_count: Generate 100 random values\nx = stats.uniform.rvs(loc=-50, scale=100, size=observation_count)\n\n# Generate Y values based on X following a linear relationship: Y = 2X + noise\n# 2 * x: This creates a linear relationship where Y increases by 2 for every\n#   unit increase in X\n# stats.norm.rvs(...): This adds random noise from a normal distribution\n# - loc=0: The noise is centered around 0 (no bias)\n# - scale=20: The standard deviation of the noise is 20\n#   (determines how scattered the points are)\n# - size=observation_count: Generate 100 random noise values\ny = 2 * x + stats.norm.rvs(loc=0, scale=20, size=observation_count)\n\ndf = pd.DataFrame({\"X\": x, \"Y\": y})\n\n# Create a scatter plot to visualize the relationship between X and Y\ndf.plot(kind=\"scatter\", x=\"X\", y=\"Y\")\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-19-output-1.png){}\n:::\n:::\n\n\nLooks like a pretty strong trend there. Let's build a linear model using ordinary least squares fitting. We can use the [ols()](https://www.statsmodels.org/dev/generated/statsmodels.formula.api.ols.html#statsmodels.formula.api.ols) function from the statsmodels package for this. This package has some neat features like the ability to specify models in a way that looks a lot like how we might do it in R.\n\n::: {#01c91fb7 .cell execution_count=19}\n``` {.python .cell-code}\n# Import necessary libraries (assumed to be done before this code)\n# statsmodels is used for statistical modeling\n# sns (seaborn) is used for data visualization\n\n# Describe the model using the formula interface\n# \"Y ~ X\" means we want to predict Y based on X (simple linear regression)\n# This creates a model object but doesn't fit it to the data yet\nmodel = statsmodels.formula.api.ols(\"Y ~ X\", data=df)\n\n# Fit the model to the data\n# This performs the actual regression calculations and returns a results object\n# containing coefficients, p-values, R-squared, etc.\nresult = model.fit()\n\n# Display a summary of the regression results\n# The summary() method provides a comprehensive report of the model performance\n# 'slim=True' parameter gives a condensed version of the output with just essential\n#   statistics\n# This will show coefficients, standard errors, t-values, p-values, R-squared, etc.\nprint(result.summary(slim=True))\n\n# Create a scatter plot with regression line using seaborn\n# lmplot automatically:\n# - Creates a scatter plot of the raw data (X vs Y)\n# - Fits a regression line\n# - Adds a shaded confidence interval around the line\n# This provides a visual representation of the relationship we just modeled\n# statistically\nsns.lmplot(df, x=\"X\", y=\"Y\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                      Y   R-squared:                       0.877\nModel:                            OLS   Adj. R-squared:                  0.875\nNo. Observations:                 100   F-statistic:                     696.5\nCovariance Type:            nonrobust   Prob (F-statistic):           2.51e-46\n==============================================================================\n                 coef    std err          t      P>|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept      2.5727      2.091      1.230      0.221      -1.576       6.722\nX              1.9263      0.073     26.391      0.000       1.781       2.071\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-20-output-2.png){}\n:::\n:::\n\n\nThe summary table gives you the main regression result info, including fitted coefficients, standard errors, p-values, R-squared values, etc. We won't get into the details of how regression is calculated or how exactly to interpret it in this chapter, so just focus on the coefficients and the p-values for now.\n\nIn this case, we have a coefficient for `X` of 1.9, which is pretty close to the true model, and it is significant! Additionally, the R-squared value is 0.877, which means this model captures a lot of the variability in our data. In other words, it's a very good model!\n\n### With No Relationship in the Data\n\nLet's try another dataset, but this time the response is not related to the predictors.\n\n::: {#1c4316b3 .cell execution_count=20}\n``` {.python .cell-code}\n# Set the seed\nnp.random.seed(962378)\n\n# Generate the data\nobservation_count = 100\nx = stats.uniform(loc=-50, scale=100).rvs(size=observation_count)\n# The response values are basically random noise\ny = stats.norm(loc=0, scale=10).rvs(size=observation_count)\ndf = pd.DataFrame({\"X\": x, \"Y\": y})\n\n# Create the model\nmodel = statsmodels.formula.api.ols(\"Y ~ X\", data=df)\n\n# Fit the model\nresult = model.fit()\n\n# Print the summary and plot the data\nprint(result.summary(slim=True))\nsns.lmplot(df, x=\"X\", y=\"Y\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                      Y   R-squared:                       0.001\nModel:                            OLS   Adj. R-squared:                 -0.009\nNo. Observations:                 100   F-statistic:                   0.08288\nCovariance Type:            nonrobust   Prob (F-statistic):              0.774\n==============================================================================\n                 coef    std err          t      P>|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept      0.3901      1.057      0.369      0.713      -1.707       2.487\nX             -0.0115      0.040     -0.288      0.774      -0.091       0.068\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-21-output-2.png){}\n:::\n:::\n\n\nThis time, we have pretty much the opposite result. The fitted coefficient is around zero, the line on the plot is basically horizontal, and the R-squared value is very low. So it is very likely that there is no relationship that we can measure between these two variables. And of course, we expected this result, as that is how we set up the data!\n\n::: {#tip-08-regression-interpretation .callout-tip title=\"Stop & Think\" collapse=\"false\"}\nWhat does an R-squared value close to zero tell you about a linear regression model?\n:::\n\n### Multiple Predictors\n\nMultiple predictors can be given as well. Let's make another data set with three predictors, $x_1$, $x_2$, and $x_3$. The response $y$ will only depend on the first two predictors, however.\n\n::: {#221160b1 .cell execution_count=21}\n``` {.python .cell-code}\nnp.random.seed(526347)\n\nobservation_count = 100\n\n# Generate 3 independent variables from uniform distributions\n# Each variable ranges from -50 to 50 (loc=-50, scale=100)\nx1 = stats.uniform(loc=-50, scale=100).rvs(size=observation_count)\nx2 = stats.uniform(loc=-50, scale=100).rvs(size=observation_count)\nx3 = stats.uniform(loc=-50, scale=100).rvs(size=observation_count)\n\n# Generate random noise from a normal distribution with mean 0 and standard\n# deviation 10\nnoise = stats.norm(loc=0, scale=10).rvs(size=observation_count)\n\n# Create the dependent variable y as a linear combination of x1 and x2,\n#   plus noise\n# True model: y = 2*x1 - 3*x2 + noise\n# - Note that x3 is not used in generating y!\ny = 2 * x1 - 3 * x2 + noise\n\n# Create a DataFrame with all variables\n# Sort it by the y values and reset the index\ndf = (\n    pd.DataFrame({\"X1\": x1, \"X2\": x2, \"X3\": x3, \"Y\": y})\n    .sort_values(\"Y\")\n    .reset_index(drop=True)\n)\n\n# Fit a linear regression model using all three predictors\n# - Formula notation: Y ~ X1 + X2 + X3\nmodel = statsmodels.formula.api.ols(\"Y ~ X1 + X2 + X3\", data=df)\nresult = model.fit()\n\n# Display a summary of the regression results\n# - slim=True omits some of the more detailed statistics\nprint(result.summary(slim=True))\n\n# Create a pairplot showing relationships between Y and each predictor\n# - kind=\"reg\" adds regression lines to the scatter plots\nsns.pairplot(df, kind=\"reg\", y_vars=\"Y\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                      Y   R-squared:                       0.990\nModel:                            OLS   Adj. R-squared:                  0.989\nNo. Observations:                 100   F-statistic:                     3044.\nCovariance Type:            nonrobust   Prob (F-statistic):           5.20e-95\n==============================================================================\n                 coef    std err          t      P>|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept      1.1886      0.986      1.205      0.231      -0.769       3.146\nX1             2.0538      0.033     62.303      0.000       1.988       2.119\nX2            -2.9983      0.037    -81.967      0.000      -3.071      -2.926\nX3             0.0445      0.034      1.318      0.191      -0.023       0.112\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-22-output-2.png){}\n:::\n:::\n\n\nThe coefficients of the variables, p-values, and scatter plots are all about what we would expect given the way we generated the data.\n\nOne cool thing to call out is the [pairplot()](https://seaborn.pydata.org/generated/seaborn.pairplot.html) function from seaborn. It's a nice way to visualize multiple variables at the same time. Here, we can see a positive association between `X1` and `y`, a negative association between `X2` and `y`, and no association between `X3` and `y`. Cool!\n\nLinear regression assumes some things about your data (are you tired of me saying this yet...). For example, the [JMP docs](https://www.jmp.com/en/statistics-knowledge-portal/what-is-regression/simple-linear-regression-assumptions) put it this way:\n\n> - The true relationship is linear\n> - Errors are normally distributed\n> - Homoscedasticity of errors (or, equal variance around the line).\n> - Independence of the observations\n\nIt's a good idea to check these assumptions before interpreting your results. There are many common [diagnostic plots](https://library.virginia.edu/data/articles/diagnostic-plots) for doing so, like the normal Q-Q plot, and the scale-location plot, but that is a story for another time!\n\n_Note: There is a really cool site that talks about how most [common statistical models are basically linear models](https://lindeloev.github.io/tests-as-linear/) (or at least, very close to them). It's a pretty neat way to think of statistical modeling that I encourage you to check out!_\n\n::: {#tip-08-feature-selection .callout-tip title=\"Stop & Think\" collapse=\"false\"}\nIn our multiple regression example, how could you determine which predictors are most important for the model?\n:::\n\n## Dimensionality Reduction & Clustering\n\nLet's switch gears a little bit and introduce dimensionality reduction and clustering. Dimensionality reduction techniques help transform complex high-dimensional data (like genomic or proteomic datasets with thousands of features) into simpler representations that capture the essential patterns. Clustering methods then help identify natural groupings within this data, allowing you to discover hidden structures without prior labeling. These techniques are common tools in bioinformatics that allow researchers to visualize complex biological relationships, identify subtypes of diseases, or group similar protein structures together when manual classification would be impractical.\n\n### Principal Components Analysis (PCA)\n\nPrincipal Component Analysis (PCA) is a widely used dimensionality reduction technique that transforms your original variables into a new set of uncorrelated variables called principal components. These principal components are ordered by how much of the original data's variance they capture, with the first component accounting for the most variance, the second component capturing the second most, and so on. PCA is more or less finding the directions (or axes) along which your data varies the most, allowing you to reduce dimensionality while preserving as much information as possible.\n\n_Note: there is another way to think of PCA, and that is as a form of [indirect gradient analysis](https://ordination.okstate.edu/PCA.htm) or as [biplots of compositional data](https://doi.org/10.1111/1467-9876.00275), which are two of my favorite topics. Unfortunately, or maybe fortunately depending on your interests, we won't be covering that here!_\n\nFirst, let's read in the data. You might recognize this dataset, its Fisher's famous iris dataset!\n\n::: {#c8da74d0 .cell execution_count=22}\n``` {.python .cell-code}\niris = pd.read_csv(\"../_data/iris.csv\")\ndisplay(iris)\n\nsns.pairplot(iris, hue=\"Species\")\n```\n\n::: {.cell-output .cell-output-display}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Sepal.Length</th>\n      <th>Sepal.Width</th>\n      <th>Petal.Length</th>\n      <th>Petal.Width</th>\n      <th>Species</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>5.1</td>\n      <td>3.5</td>\n      <td>1.4</td>\n      <td>0.2</td>\n      <td>setosa</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>4.9</td>\n      <td>3.0</td>\n      <td>1.4</td>\n      <td>0.2</td>\n      <td>setosa</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>4.7</td>\n      <td>3.2</td>\n      <td>1.3</td>\n      <td>0.2</td>\n      <td>setosa</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4.6</td>\n      <td>3.1</td>\n      <td>1.5</td>\n      <td>0.2</td>\n      <td>setosa</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5.0</td>\n      <td>3.6</td>\n      <td>1.4</td>\n      <td>0.2</td>\n      <td>setosa</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>145</th>\n      <td>6.7</td>\n      <td>3.0</td>\n      <td>5.2</td>\n      <td>2.3</td>\n      <td>virginica</td>\n    </tr>\n    <tr>\n      <th>146</th>\n      <td>6.3</td>\n      <td>2.5</td>\n      <td>5.0</td>\n      <td>1.9</td>\n      <td>virginica</td>\n    </tr>\n    <tr>\n      <th>147</th>\n      <td>6.5</td>\n      <td>3.0</td>\n      <td>5.2</td>\n      <td>2.0</td>\n      <td>virginica</td>\n    </tr>\n    <tr>\n      <th>148</th>\n      <td>6.2</td>\n      <td>3.4</td>\n      <td>5.4</td>\n      <td>2.3</td>\n      <td>virginica</td>\n    </tr>\n    <tr>\n      <th>149</th>\n      <td>5.9</td>\n      <td>3.0</td>\n      <td>5.1</td>\n      <td>1.8</td>\n      <td>virginica</td>\n    </tr>\n  </tbody>\n</table>\n<p>150 rows × 5 columns</p>\n</div>\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-23-output-2.png){}\n:::\n:::\n\n\nWe can use the [PCA](https://www.statsmodels.org/stable/generated/statsmodels.multivariate.pca.PCA.html#statsmodels.multivariate.pca.PCA) class from the statsmodels package to compute the PCA:\n\n::: {#14eed6fc .cell execution_count=23}\n``` {.python .cell-code}\nresult = PCA(\n    # The data to perform PCA on - our iris dataset without the 'Species' column\n    # We're only using the numeric feature columns\n    iris.drop(columns=\"Species\"),\n    # demean=True means that the mean will be subtracted from each feature\n    # This centers the data around zero\n    # R's `prcomp()` calls this parameter \"center\"\n    demean=True,\n    # standardize=False means that we are NOT scaling each feature to have unit variance\n    # If True, each feature would be divided by its standard deviation\n    # R's `prcomp()` would use both center = TRUE and scale. = TRUE for standardization\n    standardize=False,\n    # We won't get into the math behind this parameter.  Just know that setting it to\n    # False is the more typical use case for our purposes.\n    normalize=False,\n    # ncomp=2 specifies that we want to keep only the first 2 principal components\n    # These will be the 2 directions that capture the most variance in the data\n    ncomp=2,\n)\nresult\n```\n\n::: {.cell-output .cell-output-display execution_count=23}\n```\nPrincipal Component Analysis(nobs: 150, nvar: 4, transformation: Demean (Covariance), normalization: False, number of components: 2, SVD, id: 0x1154e30e0)\n```\n:::\n:::\n\n\nOnce PCA is performed, we can visualize the data by plotting the projections onto the first two principal components:\n\n::: {#ba9abe62 .cell execution_count=24}\n``` {.python .cell-code}\n# Prepare the data for plotting\nplot_data = (\n    result.scores.rename(\n        columns={\n            # Rename the first principal component for better readability\n            \"comp_0\": \"PC 1\",\n            # Rename the second principal component for better readability\n            \"comp_1\": \"PC 2\",\n        }\n    )\n    # Add the Species column from the original dataset for coloring points\n    .assign(Species=iris[\"Species\"])\n)\n\n# Create a scatter plot using seaborn\n# - relplot creates a relational plot (scatter plot in this case)\n# - x and y specify which columns to use for the x and y axes\n#   (the first two principal components)\n# - hue colors the points based on the Species column\nsns.relplot(plot_data, x=\"PC 1\", y=\"PC 2\", hue=\"Species\")\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-25-output-1.png){}\n:::\n:::\n\n\nAs you can see, the three species are fairly well separated along the x-axis, which represents the first principal component.\n\n::: {#tip-08-pca-interpretation .callout-tip title=\"Stop & Think\" collapse=\"false\"}\nWhat biological interpretation might you give to the first principal component in this iris dataset analysis?\n:::\n\nThere is a lot more to say about PCA, including differences between using the covariance vs. correlation matrix, biplots, interpretation, etc., but we will leave it there for now.\n\n### k-means Clustering\n\nK-means clustering is a common unsupervised machine learning technique that partitions your data into a specified number, `k`, of distinct, non-overlapping groups, without requiring labeled training data. The algorithm works by iteratively assigning data points to the nearest cluster center, then recalculating those centers until convergence is reached. Some use cases for k-means clustering include grouping genes given expression data, identifying protein function similarities, or finding patient subgroups with similar clinical profiles.\n\nWe can use SciPy's [kmeans2()](https://docs.scipy.org/doc/scipy/reference/generated/scipy.cluster.vq.kmeans2.html) function to run the k-means algorithm on our iris data:\n\n::: {#9b75da65 .cell execution_count=25}\n``` {.python .cell-code}\n# Set the random seed for reproducibility\nnp.random.seed(238974)\n\n# Perform K-means clustering on the iris dataset\n# - We're using the numeric columns of the iris data frame\n# - We specify k=3 to create 3 clusters (matching the number of iris species)\n# - We ignore the _centroids\n# - `labels` will contain the cluster assignment (0, 1, or 2) for each iris sample\n_centroids, labels = cluster.vq.kmeans2(iris.drop(columns=\"Species\"), k=3)\n\n# Create a scatter plot to visualize the clustering results\n# - We're plotting the first two principal components (PC 1 vs PC 2)\n# - Each point is colored by its actual species (using 'hue')\n# - Each point's marker style is determined by its cluster assignment (using 'style')\n# - This allows us to compare the actual species classifications with the clustering\n#   results\n# - 'plot_data.assign(Cluster=labels)' adds the cluster labels as a new column to the\n#   data frame\nsns.relplot(\n    plot_data.assign(Cluster=labels),\n    x=\"PC 1\",\n    y=\"PC 2\",\n    hue=\"Species\",\n    style=\"Cluster\",\n    legend=\"full\",\n)\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-26-output-1.png){}\n:::\n:::\n\n\nWe can see that there is pretty good overlap between the clusters assigned by the k-means algorithm with the true species groupings. In most cases, you won't get clusters that perfectly represent your data. For this reason, there are a ton of techniques to help you evaluate and optimize clustering and other classification methods that we won't get into here. Instead, we will keep it super simple and just look at the proportion of cluster labels for each species. This will give us a rough idea about the quality of the clustering results.\n\n::: {#21100fda .cell execution_count=26}\n``` {.python .cell-code}\ndef proportions(clusters):\n    # Count occurrences of each cluster value\n    counts = Counter(clusters)\n    # Get total number of items in clusters\n    total = len(clusters)\n    # Calculate proportion for each cluster and sort by cluster id\n    proportions = {cluster: count / total for cluster, count in sorted(counts.items())}\n    return proportions\n\n\n# For each species in the iris dataset:\n# - Assign the cluster labels to a new column 'Cluster'\n# - Group by 'Species'\n# - Aggregate the 'Cluster' column using the proportions function\n#\n# This shows the distribution of cluster assignments within each species\niris.assign(Cluster=labels).groupby(\"Species\").agg({\"Cluster\": proportions})\n```\n\n::: {.cell-output .cell-output-display execution_count=26}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Cluster</th>\n    </tr>\n    <tr>\n      <th>Species</th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>setosa</th>\n      <td>{1: 1.0}</td>\n    </tr>\n    <tr>\n      <th>versicolor</th>\n      <td>{0: 0.06, 2: 0.94}</td>\n    </tr>\n    <tr>\n      <th>virginica</th>\n      <td>{0: 0.74, 2: 0.26}</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\nThat's pretty good! The k-means clustering pretty well describes the _setosa_ and _versicolor_ species, but the _virginica_ species gets mislabelled about 25% of the time.\n\n::: {#tip-08-clustering-challenges .callout-tip title=\"Stop & Think\" collapse=\"false\"}\nWhat might explain why k-means clustering performed well for _setosa_ and _versicolor_ but less well for _virginica_?\n:::\n\n## Summary\n\nThis chapter covered key statistical and modeling techniques in Python used in life sciences. We looked at methods for comparing groups, including t-tests, ANOVA, and non-parametric alternatives, with a focus on understanding p-values and effect sizes. We also introduced linear regression for exploring relationships between variables, as well as dimensionality reduction and clustering for identifying patterns in data. Throughout, we highlighted the importance of understanding the assumptions behind these methods and interpreting results carefully. Effective data analysis goes beyond running code--it involves choosing the right methods, making sense of the results, and linking them to biological questions. Python offers a range of tools that, when used thoughtfully, support meaningful and clear analysis of experimental data.\n\n## Practice Problems\n\n### 8.1 {#sec-problem-8.1}\n\nWrite code to generate two groups of 30 samples each, where group A has a mean of 10 and standard deviation of 2, and group B has a mean of 15 and standard deviation of 2. Then perform an unpaired t-test to compare them.\n\n### 8.2 {#sec-problem-8.2}\n\nAdd a third group (mean 11, standard deviation of 2) to the dataset from the last problem. Then perform an ANOVA to determine if there are statistically significant differences between them.\n\n### 8.3 {#sec-problem-8.3}\n\nRun Tukey's HSD on the data from @sec-problem-8.2.\n\n### 8.4 {#sec-problem-8.4}\n\nGenerate a dataset with one response variable and two predictor variables, where only one predictor has a relationship with the response. Fit a multiple regression model and determine which predictor is significant.\n\n### 8.5 {#sec-problem-8.5}\n\nPerform k-means clustering with k=2 on the iris dataset. Does this grouping make biological sense? Justify your answer with a visualization.\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\" integrity=\"sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js\" integrity=\"sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==\" crossorigin=\"anonymous\" data-relocate-top=\"true\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n"
      ]
    }
  }
}